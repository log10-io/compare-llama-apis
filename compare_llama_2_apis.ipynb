{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b14adb9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec92d1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save api keys in .env file\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb6289d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import llama-2 api libs\n",
    "import os\n",
    "\n",
    "# access crusoe self-hosted llama-2  \n",
    "# setup self-hosting Llama-2 with https://github.com/log10-io/llama\n",
    "# truthful_qa use `python3 ./api.py --host 0.0.0.0 --model 70b-chat --max_gen_len 2 --temperature 0`\n",
    "# summ_cnn use `python3 ./api.py --host 0.0.0.0 --model 70b-chat --max_gen_len 128 --temperature 0.3`\n",
    "import requests\n",
    "CRUSOE_URL = os.environ['CRUSOE_URL']\n",
    "url = f\"http://{CRUSOE_URL}/chat/completions\"\n",
    "headers = {\n",
    "    \"Authorization\": \"Bearer \" + os.environ['CRUSOE_LLAMA_SECRET'],\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "from mcli import predict as mosaicml_llm\n",
    "\n",
    "from langchain.llms import Replicate\n",
    "\n",
    "import together\n",
    "together.api_key = os.environ['TOGETHER_API_KEY']\n",
    "\n",
    "from octoai.client import Client\n",
    "\n",
    "OCTOAI_TOKEN = os.environ.get('OCTOAI_TOKEN')\n",
    "# The client will also identify if OCTOAI_TOKEN is set as an environment variable\n",
    "client = Client(token=OCTOAI_TOKEN)\n",
    "\n",
    "# need to change to your own url on octoml plateform\n",
    "llama2_70b_url = os.environ['CRUSOE_LLAMA_70B_URL']\n",
    "llama2_70b_health_url = os.environ['CRUSOE_LLAMA_70B_HEALTH_URL']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e052ad41",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d056cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "truthful_qa_file = 'helm_truthful_qa_scenario_state.json'\n",
    "\n",
    "with open(truthful_qa_file, 'r') as file:\n",
    "    dataset = json.load(file)\n",
    "\n",
    "requests_truthful_qa= [r for r in dataset['request_states'] if not r['instance'].get('perturbation')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a31353f",
   "metadata": {},
   "outputs": [],
   "source": [
    "summ_file_path = 'helm_summ_cnn_scenario_state.json'\n",
    "with open(summ_file_path, 'r') as file:\n",
    "    summ_data = json.load(file)\n",
    "\n",
    "ext_string = \"\\n\\nSummarize the above article in 3 sentences.\"\n",
    "requests_summ = [r for r in summ_data['request_states']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06bd860",
   "metadata": {},
   "source": [
    "### Define function for providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e79239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_completion(provider, prompt, system_prompt=None, hparams: dict = None):\n",
    "    full_prompt = \"[INST] <<SYS>>\\n\" + system_prompt + \"\\n<</SYS>>\\n\" + prompt + \" [/INST]\\n\"\n",
    "\n",
    "    if provider == \"crusoe\":\n",
    "        # the hparams are set by the self-hosting server\n",
    "        data = {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system_prompt\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt + \"Sure! I would select answer\"\n",
    "                }\n",
    "            ],\n",
    "            **hparams\n",
    "        }\n",
    "\n",
    "        # Make the POST request\n",
    "        response = requests.post(url, json=data, headers=headers)\n",
    "        completion = response.json()['choices'][0]['message']['content'].lstrip()\n",
    "    elif provider == \"mosaicml\":\n",
    "        response = mosaicml_llm(\"https://models.hosted-on.mosaicml.hosting/llama2-70b-chat/v1\", \n",
    "                                {\n",
    "                                    \"inputs\": [prompt],\n",
    "                                    \"parameters\": hparams\n",
    "                                })\n",
    "        completion = response['outputs'][0]\n",
    "    elif provider == \"octoml\":\n",
    "        inputs = {\n",
    "        \"model\": \"llama-2-70b-chat\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt\n",
    "            },\n",
    "            {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"stream\": False,\n",
    "        \"max_tokens\": hparams[\"max_tokens\"],\n",
    "        \"temperature\": hparams[\"temperature\"],\n",
    "        }\n",
    "\n",
    "        # For llama2, you'll replace the quickstart template endpoint URL.\n",
    "        if client.health_check(llama2_70b_health_url) == 200:\n",
    "            outputs = client.infer(endpoint_url=llama2_70b_url, inputs=inputs)\n",
    "\n",
    "        # Parse Llama2 outputs and print\n",
    "        completion = outputs.get('choices')[0].get(\"message\").get('content').lstrip()\n",
    "    elif provider == \"replicate\":\n",
    "        replicate_llm = Replicate(\n",
    "            model=\"meta/llama-2-70b-chat:35042c9a33ac8fd5e29e27fb3197f33aa483f72c2ce3b0b9d201155c7fd2a287\",\n",
    "            model_kwargs=hparams,\n",
    "            prompt_key=\"prompt\")\n",
    "\n",
    "        completion = replicate_llm(full_prompt).lstrip()\n",
    "    elif provider == \"together\":\n",
    "        output = together.Complete.create(\n",
    "            prompt = prompt, \n",
    "            model = \"togethercomputer/llama-2-70b-chat\", \n",
    "            max_tokens = hparams['max_tokens'],\n",
    "            temperature = hparams['temperature'],\n",
    "        )\n",
    "        completion = output['output']['choices'][0]['text']\n",
    "    else:\n",
    "        print(\"INVALID PROVIDER\")\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca31a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "truthful_qa_system_prompt = \"\"\"You will be presented with a question followed by multiple answer choices. Your task is to identify and provide the single most accurate and truthful answer from the given choices for the last question. Remember to base your response strictly on factual accuracy. Only return the choice, like A, B, C, etc. Don't explain or say anything else.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6778db0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "providers_for_truthful_qa = {\n",
    "    \"crusoe\": {\"hparams\": {\"temperature\": 0}, \"system_prompt\": truthful_qa_system_prompt},\n",
    "    \"mosaicml\": {\"hparams\": {\"temperature\": 0, \"max_new_tokens\": 1, \"top_p\": 1}, \"system_prompt\": \"\"},\n",
    "    \"octoml\": {\"hparams\": {\"temperature\": 0, \"max_tokens\": 2}, \"system_prompt\": truthful_qa_system_prompt},\n",
    "    \"replicate\": {\"hparams\": {\"temperature\": 0.01, \"max_new_tokens\": 2}, \"system_prompt\": truthful_qa_system_prompt}, # Replicate temperature min value is 0.01\n",
    "    \"together\": {\"hparams\": {\"temperature\": 0, \"max_tokens\": 1}, \"system_prompt\": \"\"},\n",
    "}\n",
    "providers_for_summ = {\n",
    "    \"crusoe\": {\"hparams\": {\"temperature\": 0.3}, \"system_prompt\": \"\"},\n",
    "    \"mosaicml\": {\"hparams\": {\"temperature\": 0.3, \"max_new_tokens\": 128}, \"system_prompt\": \"\"},\n",
    "    \"octoml\": {\"hparams\": {\"temperature\": 0.3, \"max_tokens\": 128}, \"system_prompt\": \"\"},\n",
    "    \"replicate\": {\"hparams\": {\"temperature\": 0.3, \"max_new_tokens\": 128}, \"system_prompt\": \"\"},\n",
    "    \"together\": {\"hparams\": {\"temperature\": 0.3, \"max_tokens\": 128}, \"system_prompt\": \"\"},\n",
    "}\n",
    "benchmark_map = {\"truthful_qa\": {\"data\": requests_truthful_qa, \"providers\": providers_for_truthful_qa},\n",
    "            \"summ_cnn\": {\"data\": requests_summ, \"providers\": providers_for_summ},}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a5662b",
   "metadata": {},
   "source": [
    "### Run the tests with providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5bed4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the reference answer for truthful qa\n",
    "def get_reference_answer(request):\n",
    "    for ref in request['instance']['references']:\n",
    "        if 'correct' in ref['tags']:\n",
    "            correct_ans = ref['output']['text']\n",
    "\n",
    "    for k, v in request['output_mapping'].items():\n",
    "        if v == correct_ans:\n",
    "            return k\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae04a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "output_dir = \"./benchmark_outputs\"\n",
    "for bench in benchmark_map.keys():\n",
    "    if not os.path.exists(f\"{output_dir}/{bench}\"):\n",
    "        os.makedirs(f\"./benchmark_outputs/{bench}\")\n",
    "\n",
    "    for provider in benchmark_map[bench]['providers'].keys():\n",
    "\n",
    "        filename = f\"{output_dir}/{bench}/results_{bench}_{provider}.csv\"\n",
    "        data = benchmark_map[bench]['data']\n",
    "        provider_kwargs = benchmark_map[bench]['providers'][provider]\n",
    "\n",
    "        with open(filename, 'w', newline='') as csvfile:\n",
    "            # Create CSV writer object\n",
    "            csvwriter = csv.writer(csvfile)\n",
    "            \n",
    "            csvwriter.writerow(['ID','Completion Time', 'Completion', 'Expected'])\n",
    "\n",
    "            count = 0\n",
    "            for req in tqdm(data[:], \"completed requests\"):\n",
    "                prompt = req['request']['prompt']\n",
    "                tik = time.time()\n",
    "                response = llama_completion(prompt=prompt, system_prompt=\"\", provider=provider, hparams=provider_kwargs['hparams'])\n",
    "                elapsed_time = time.time() - tik\n",
    "                id = req['instance']['id']\n",
    "                if bench == \"truthful_qa\":\n",
    "                    expected = get_reference_answer(req)\n",
    "                elif bench == \"summ_cnn\":\n",
    "                    expected = req['instance']['references'][0]['output']['text']\n",
    "                \n",
    "                # Write results to CSV\n",
    "                csvwriter.writerow([id, elapsed_time, response, expected])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4599cb5c",
   "metadata": {},
   "source": [
    "### Process results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56fff15",
   "metadata": {},
   "source": [
    "#### Summ_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f612ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "code for summ_cnn results evaluation\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import os\n",
    "from rouge import Rouge\n",
    "rouge = Rouge()\n",
    "\n",
    "folder_name = \"./benchmark_outputs/summ_cnn/\"\n",
    "csv_files = [f for f in os.listdir(folder_name) if f.endswith(\".csv\")]\n",
    "\n",
    "results = []\n",
    "row_names = []\n",
    "for file in csv_files:\n",
    "    row_name = file.split('_')[-1].split('.')[0]\n",
    "    df_results = pd.read_csv(folder_name + file)\n",
    "    completion = df_results['Completion'].tolist()\n",
    "    reference = df_results['Expected'].tolist()\n",
    "    # call rogue to get the scores for the list completion with reference ref\n",
    "    scores = rouge.get_scores(completion, reference, avg=True)\n",
    "\n",
    "    avg_completion_time = df_results['Completion Time'].mean()\n",
    "    sem_completion_time = df_results['Completion Time'].sem()\n",
    "    results.append([scores['rouge-1']['f'], scores['rouge-2']['f'], scores['rouge-l']['f'], avg_completion_time, sem_completion_time])\n",
    "    row_names.append(row_name)\n",
    "\n",
    "# Convert results to a DataFrame and display\n",
    "summary_df = pd.DataFrame(results, columns=['ROUGE-1(F1)', 'ROUGE-2(F1)', 'ROUGE-L(F1)', 'Completion_time(S)', 'SEM'], index=row_names)\n",
    "print(summary_df.sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3d71e4",
   "metadata": {},
   "source": [
    "#### TruthfulQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75070e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "code for truthful qa results evaluation\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import os\n",
    "# Disable the warning\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "folder_name = \"./benchmark_outputs/truthful_qa/\"\n",
    "# List all CSV files in the directory\n",
    "csv_files = [f for f in os.listdir(folder_name) if f.endswith('.csv')]\n",
    "\n",
    "results = []\n",
    "row_names = []\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(folder_name + file)\n",
    "    row_name = file.split('_')[-1].split('.')[0]\n",
    "    \n",
    "    avg_completion_time = df['Completion Time'].mean()\n",
    "    sem_completion_time = df['Completion Time'].sem()\n",
    "    correct_rate = (df['Completion'] == df['Expected']).sum() / len(df)\n",
    "\n",
    "    results.append([avg_completion_time, sem_completion_time, correct_rate])\n",
    "    \n",
    "    # Extract the identifier from the filename and add to row names\n",
    "    row_names.append(row_name)\n",
    "\n",
    "# Convert results to a DataFrame and display\n",
    "summary_df = pd.DataFrame(results, columns=['Avg Completion Time', 'SEM', 'Correct Rate'], index=row_names)\n",
    "print(summary_df.sort_index())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
